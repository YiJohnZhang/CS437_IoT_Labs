{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73d1535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3dcd1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"dataset/train\"\n",
    "val_dir = \"dataset/test\"\n",
    "img_size = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0d835c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3088 files belonging to 4 classes.\n",
      "Found 4 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25db97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving Generalization Ability\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6296d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 2. Load the pre-trained MobileNetV2 model (excluding the final classification layer)\n",
    "# --------------------\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False, \n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False   # Freeze convolutional layer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4724f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 3. Build a new model\n",
    "# --------------------\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)  # Prevent overfitting\n",
    "outputs = layers.Dense(len(train_ds.class_names), activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7438ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 4. Compile model\n",
    "# --------------------\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdfbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 232ms/step - accuracy: 0.5396 - loss: 1.1521 - val_accuracy: 1.0000 - val_loss: 0.2195\n",
      "Epoch 2/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy: 0.9999 - loss: 0.1214 - val_accuracy: 1.0000 - val_loss: 0.0818\n",
      "Epoch 3/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 0.0461 - val_accuracy: 1.0000 - val_loss: 0.0454\n",
      "Epoch 4/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 197ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 1.0000 - val_loss: 0.0299\n",
      "Epoch 5/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0219\n",
      "Epoch 6/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 0.0170\n",
      "Epoch 7/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 0.0134\n",
      "Epoch 8/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0111\n",
      "Epoch 9/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0093\n",
      "Epoch 10/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
      "Epoch 11/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0069\n",
      "Epoch 12/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0060\n",
      "Epoch 13/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0053\n",
      "Epoch 14/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0048\n",
      "Epoch 15/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
      "Epoch 16/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
      "Epoch 17/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
      "Epoch 18/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 199ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
      "Epoch 19/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 20/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
      "Epoch 21/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
      "Epoch 22/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 188ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
      "Epoch 23/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 24/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 9.3641e-04 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 25/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 8.2313e-04 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 26/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 7.9346e-04 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 27/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 7.1952e-04 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 28/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 7.1272e-04 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 29/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 6.2944e-04 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 30/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 6.0177e-04 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 31/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 5.3689e-04 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 32/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 181ms/step - accuracy: 1.0000 - loss: 5.2618e-04 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 33/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 4.8492e-04 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 34/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 4.4729e-04 - val_accuracy: 1.0000 - val_loss: 9.7110e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 4.2754e-04 - val_accuracy: 1.0000 - val_loss: 9.1321e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 3.9603e-04 - val_accuracy: 1.0000 - val_loss: 8.6192e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 3.7514e-04 - val_accuracy: 1.0000 - val_loss: 8.1248e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 3.4218e-04 - val_accuracy: 1.0000 - val_loss: 7.6628e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 3.1891e-04 - val_accuracy: 1.0000 - val_loss: 7.2391e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 1.0000 - loss: 3.0409e-04 - val_accuracy: 1.0000 - val_loss: 6.8238e-04\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# 5. Train\n",
    "# --------------------\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b77270f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The model has been trained and saved: mobilenetv2_custom.keras\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# 6. Save Model\n",
    "# --------------------\n",
    "\n",
    "model.save(\"mobilenetv2_custom.keras\")\n",
    "\n",
    "print(\"✅ The model has been trained and saved: mobilenetv2_custom.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2660323",
   "metadata": {},
   "source": [
    "Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dccabef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop sign', 'traffic cone', 'traffic lights', 'walker']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55030bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Result: stop sign\n",
      "[[9.9909723e-01 1.8722552e-04 2.7068341e-04 4.4489323e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Read Picture\n",
    "img_path = \"dataset/test/1.png\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(img_array)\n",
    "class_names = train_ds.class_names\n",
    "print(\"Result:\", class_names[np.argmax(pred)])\n",
    "\n",
    "print(pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
